{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rabbitmetrics/langchain-13-min/blob/main/notebooks/langchain-13-min.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "50dvxjqCFmhF"
      },
      "outputs": [],
      "source": [
        "# Imports\n",
        "from langchain import PromptTemplate\n",
        "from langchain.chains import LLMChain, SimpleSequentialChain\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.vectorstores import Chroma\n",
        "\n",
        "from factory import Models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C7RnyUOCJWmk"
      },
      "outputs": [],
      "source": [
        "# Create an LLM\n",
        "models = Models()\n",
        "llm = models.llama()\n",
        "llm(\"explain large language models in one sentence\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2grf7I8AJ_hK"
      },
      "outputs": [],
      "source": [
        "# Run LLM with PromptTemplate\n",
        "\n",
        "template = \"\"\"\n",
        "You are an expert data scientist with an expertise in building deep learning models. \n",
        "Explain the concept of {concept} in a couple of lines\n",
        "\"\"\"\n",
        "prompt = PromptTemplate(\n",
        "    input_variables=[\"concept\"],\n",
        "    template=template,\n",
        ")\n",
        "\n",
        "llm(prompt.format(concept=\"autoencoder\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dm78i-rUKXIB"
      },
      "outputs": [],
      "source": [
        "# Define chains whose output from a chain is input to the next chain\n",
        "\n",
        "chain = LLMChain(llm=llm, prompt=prompt)\n",
        "chain.run(\"autoencoder\")\n",
        "\n",
        "# Define another chain\n",
        "second_prompt = PromptTemplate(\n",
        "    input_variables=[\"ml_concept\"],\n",
        "    template=\"Turn the concept description of {ml_concept} and explain it to me like I'm five in 500 words\",\n",
        ")\n",
        "chain_two = LLMChain(llm=llm, prompt=second_prompt)\n",
        "chain_two.run(\"algorithm_concept\")\n",
        "\n",
        "# Sequential chain: first chain's output is second chain's input\n",
        "overall_chain = SimpleSequentialChain(chains=[chain, chain_two], verbose=True)\n",
        "explanation = overall_chain.run(\"autoencoder\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mDDu1B_SLQls"
      },
      "outputs": [],
      "source": [
        "# Import utility for splitting up texts and split up the explanation given above into document chunks\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size = 200,\n",
        "    chunk_overlap  = 50,\n",
        ")\n",
        "documents = text_splitter.create_documents([explanation])\n",
        "\n",
        "# Document structure\n",
        "documents[-1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z5sv4e3tLw2y"
      },
      "outputs": [],
      "source": [
        "# Reference: https://python.langchain.com/docs/modules/data_connection/vectorstores/\n",
        "embeddings = models.llama_embeddings()\n",
        "\n",
        "# LangChain embeddings offer two base methods: embed_documents and embed_query.\n",
        "embeddings.embed_documents([doc.page_content for doc in documents])\n",
        "embeddings.embed_query(\n",
        "    \"What is magical about an autoencoder?\"\n",
        ")\n",
        "\n",
        "# There are many ways to create a vector store.\n",
        "# Here we use the Chroma library to create a vector store in memory.\n",
        "# TODO: failed here saying the document's metadata is empty\n",
        "db = Chroma.from_documents(documents, embeddings)\n",
        "\n",
        "# Do a simple vector similarity search\n",
        "query = \"What is magical about an autoencoder?\"\n",
        "result = db.similarity_search(query)\n",
        "\n",
        "print(result)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyONM96f7/m0jUCD9c87+MQy",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
